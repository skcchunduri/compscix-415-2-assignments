---
title: "COMPSCIX 415.2 Homework 5/Midterm"
author: "Chunduri SriKeerthi Chandra"
date: "July 10th 2018"
output: 
  html_document:
    self_contained: true
    theme: united
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r initilize, include=FALSE}
library(tidyverse)
library(knitr)
```

My Github repository for my assignments can be found at this URL: [https://github.com/skcchunduri/compscix-415-2-assignments.git](https://github.com/skcchunduri/compscix-415-2-assignments.git)

### <span style="color:blue">The tidyverse packages</span>

#### *1Q) Can you name which package is associated with each task below?*
```{r echo=FALSE}
pkgs <- tribble(
  ~Package,             ~Task,
  #-----------------|--------
  "ggplot2",   "Plotting",
  "dplyr",   "Data munging/wrangling",
  "tidyr",   "Reshaping (speading and gathering) data",
  "readr", "Importing/exporting data"
)
kable(pkgs)
```


#### *2Q) Can you name two functions that you’ve used from each package that you listed above for these tasks*

1. Plotting :
    a) geom_point() used to create scatter plots
        ```{r eval=FALSE}
          #Plot a scatterplot for highway mileage Vs no. of cylinders in a vehicle
          ggplot(data = mpg) +
            geom_point(mapping = aes(x = hwy, y = cyl))
        ```
    b) geom_bar() used to create bar charts
        ```{r eval=FALSE}
          #Plot a bar chart for cut Vs number of diamonds
          ggplot(data = diamonds) + 
            geom_bar(mapping = aes(x = cut))
        ```
2. Data munging/wrangling
    a) filter() used to subset the observations based on the values.
        ```{r eval=FALSE}
          #All flights that departed in November or December
          filter(flights, month == 11 | month == 12)
        ```
    b) arrange() It takes a data frame and a set of column names to order by.
        ```{r eval=FALSE}
          #Flights arranged according to year/month/day
          arrange(year, month, day)
        ```
3. Reshaping data
    a) gather() makes wide tables narrower and longer
        ```{r eval=FALSE}
          #Gather observations under 1999 and 2000 columns in to year and population columns (variables)
          gather(`1999`, `2000`, key = "year", value = "population")
        ```
    b) spread() makes long tables shorter and wider
        ```{r eval=FALSE}
          #Spread observations under "type"" in to columns,
          #where as the column that contains values from multiple variables is "count""
          spread(key = type, value = count)
        ```
4. Importing/exporting data
    a) read_csv() reads comma delimited files
        ```{r eval=FALSE}
          #Read the comma seperated file content in to heights
          heights <- read_csv(file = "data/heights.csv")
        ```
    b) write_csv() writes comma delimited files
        ```{r eval=FALSE}
          #Write file content @ "fileData" in to baby_names.csv
          csvFilePath <- "/Lab5/baby_names.csv"
          fileData %>% write_csv(path = csvFilePath)
        ```

###<span style="color:blue">R Basics</sapn>

#### *1Q) Fix this code with the fewest number of changes possible so it works*

```{r eval=FALSE}
  My_data.name___is.too00ooLong! <- c( 1 , 2 , 3 )
```

Some of the naming convensions followed (and or allowed) in R:
  
  * alllowercase
  * period.separated
  * underscore_separated 
  * lowerCamelCase
  * UpperCamelCase

**!** is a special character for **R** interpreter.

```{r}
My_data.name___is.too00ooLong <- c( 1 , 2 , 3 )
```

#### *2Q) Fix this code so it works:*
```{r eval=FALSE}
my_string <- C('has', 'an', 'error', 'in', 'it)
```

**c()** is a function that combines its arguments to generate a vector. And also opening brace should match the closing brace.

```{r}
my_string <- c('has', 'an', 'error', 'in', 'it')
my_string
```

#### *3Q) Look at the code below and comment on what happened to the values in the vector.*
```{r}
my_vector <- c(1, 2, '3', '4', 5)
my_vector
```

All arguments are coerced to a common string type because '3' and '4' are strings.

###<span style="color:blue">Data import/export</span>

#### *1Q) Download the rail_trail.txt file from Canvas (in the Midterm Exam section) and successfully import it into R. Prove that it was imported successfully by including your import code and taking a glimpse of the result.*

```{r}
#File path to rail_trail.txt
filePath <- "/Users/srchundu/UCBExtension/DataScienceIntro/compscix-415-2-assignments/Lab5/rail_trail.txt"

#File is a "|" seperated, load using read_delim
fileData <- read_delim(file = filePath, delim = "|")

#Contents are
glimpse(fileData)
```

#### *2Q) Export the file into a comma-separated file and name it “rail_trail.csv”. Make sure you define the path correctly so that you know where it gets saved. Then reload the file. Include your export and import code and take* *another glimpse*

```{r}
# File path where .csv file is saved.
filePath <- "/Users/srchundu/UCBExtension/DataScienceIntro/compscix-415-2-assignments/Lab5/rail_trail.csv"

#Using write_csv to write comma-seperated file
fileData %>% write_csv(path = filePath)

# Read back baby names from CSV file
fileData2 <- read_csv(file = filePath)

#Glimpse at file content
glimpse(fileData2)
```

###<span style="color:blue">Visualization</span>

#### *1Q) Critique this graphic: give only three examples of what is wrong with this graphic. Be concise*

  1. A bar chart should have been used as opposed to the bubble chart.
  2. Under 45, 45-65 and 65+ should have used gender as a third dimension.
  3. Sum of percentages depicted doesn't add up to 100%.
  
#### *2Q) Reproduce this graphic using the diamonds data set*
```{r}
diamonds %>% ggplot(mapping = aes(x = cut, y = carat,fill = color)) + geom_boxplot(position = position_identity()) + xlab("CUT OF DIAMOND") + ylab("CARAT OF DIAMOND") + coord_flip()
```

#### *3Q) The previous graphic is not very useful. We can make it much more useful by changing one thing about it. Make the change and plot it again.*
```{r}
diamonds %>% ggplot(mapping = aes(x = cut, y = carat,fill = color)) + geom_boxplot(position = position_dodge()) + xlab("CUT OF DIAMOND") + ylab("CARAT OF DIAMOND") + coord_flip()
```


###<span style="color:blue">Data munging and wrangling</span>

#### *1Q) Is this data “tidy”? If yes, leave it alone and go to the next problem. If no, make it tidy. Note: this data set is called table2 and is available in the tidyverse package. It should be ready for you to use after you’ve loaded the tidyverse package.*
```{r}
table2
```

No, the data is not tidy. There are three interrelated rules which make a dataset tidy:

  * Each variable must have its own column.
  * Each observation must have its own row.
  * Each value must have its own cell.
    
In case of table2 each observation is spread across multiple rows. table2 should be made shorter and wider
```{r}
table2 %>%
    spread(key = type, value = count)
```

#### *2Q) Create a new column in the diamonds data set called price_per_carat that shows the price of each diamond per carat (hint: divide). Only show me the code, not the output.*

**mutate()** allows to add new columns that are functions of existing columns, but these function must be vectorized.

```{r eval=FALSE}
diamonds %>% mutate(price_per_carat = price/carat)
```

#### *3Q) For each cut of diamond in the diamonds data set, how many diamonds, and what proportion, have a price > 10000 and a carat < 1.5? There are several ways to get to an answer, but your solution must use the data wrangling verbs from the tidyverse in order to get credit*

  * Do the results make sense? Why?
  * Do we need to be wary of any of these numbers? Why?
  
```{r}
diamonds %>% group_by(cut) %>% summarise(diamonds = n(), `price > 1000 and carat < 1.5` = sum(price > 1000 & carat < 1.5), prapotion = `price > 1000 and carat < 1.5`/n())
```

The resulting table is useful. The table summarizes that majority of the diamonds are 1.5 carat or below and are priced at more than $1000. We can also assume diamonds with carat < 1.5 are more popular than any. However, be adviced that it doesn't consider other aspects of a diamond (clarity, depth etc) that affect the price. So, there is nothing to be worried of.

###<span style="color:blue">EDA</span>

#### *1Q) During what time period is this data from?*

Txhousing data frame captures 8602 observations from Jan-2000 till July-2015.

#### *2Q) How many cities are represented?*

A total of 46 cities are represented in the Txhousing data frame.

#### *3Q) Which city, month and year had the highest number of sales?*

Houstan had 8945 sales in July 2015.

#### *4Q) What kind of relationship do you think exists between the number of listings and the number of sales? Check your assumption and show your work.*

the number of sales is linearly proportional to the number of listings. But, when the number of listings near 30000 number of sales starts dropping.

```{r}
txhousing %>% filter(!is.na(sales), !is.na(listings)) %>% ggplot(mapping = aes(x = sales, y = listings)) + geom_smooth()
```

#### *5Q) What proportion of sales is missing for each city*

```{r}
txhousing %>% group_by(city) %>% summarise(total_sales = sum(sales, na.rm = TRUE), total_missing = sum(is.na(sales)), prop_of_missing_sales = total_missing/total_sales)
```


#### *6Q) Looking at only the cities and months with greater than 500 sales:*

  * Are the distributions of the median sales price (column name median), when grouped by city,
different? The same? Show your work.

They are different:
  
```{r}
        txhousing %>% filter(sales > 500) %>%
          group_by(city) %>% ggplot(mapping = aes(x = city, y =     median)) +
           geom_boxplot() +
            coord_flip()
```

  * Any cities that stand out that you’d want to investigate further?
  
 Yes, Corpus Christi TX median prices hasn't varied much.
 
  * Why might we want to filter out all cities and months with sales less than 500?
  
filtering out the groups with the smallest numbers of observations helps identify more of the pattern and less of the extreme variation in the smallest groups.